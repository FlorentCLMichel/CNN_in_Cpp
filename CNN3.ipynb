{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple C++ implementation of a convolutional neural network\n",
    "\n",
    "<p style=\"color:blue\"><b><i> Work in progress </i></b></p>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Example of use on the MNIST dataset\n",
    "\n",
    "In this section we show how to use on the example of the MNIST dataset. \n",
    "We do not aim for efficiency here, and choose hyperparameters which demaonstrate all the features of the implementation while keeping the resulting network relatively simple. \n",
    "In the next section, we will tune the hyperparameters to improve efficiency.\n",
    "\n",
    "Our CNN implementation is found in the module `CNN3_cpp.so`. \n",
    "To use the MNIST dataset, we need to load the `mnist` module. \n",
    "We also load functions from the `numpy` and `os` modules we will use later, and the `pyplot` module to do plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CNN3\n",
    "import mnist \n",
    "from numpy import newaxis\n",
    "from numpy.random import permutation as np_permutation\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a few parameters. \n",
    "The string `save_file_bame` is the name of the file in which the CNN will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file_name = 'MNIST_CNN_1.cnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters relate to the training and testing phases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_images = 60000 # number of images used for the training\n",
    "n_epochs = 4 # number of epochs\n",
    "n_test_images = 10000 # number of images used for the test\n",
    "n_print = 4000 # The accuracy is printed every n_print training steps.\n",
    "learn_rate = 0.01 # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the hyperparameters of the network. \n",
    "Here we work with a CNN having 2 convolution layers and 2 fully-connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w = 28 # width of input images\n",
    "img_h = 28 # height of input images\n",
    "CL_size_filters = [5,3] # list of filter sizes for the convolution layers\n",
    "CL_num_filters = [16,32] # number of filters for the convolution layers\n",
    "MP_size = [2,2] # size of the maxpool layers\n",
    "FC_size = [32] # number of neurons for the cully-connected layers (exclusing the output lauer)\n",
    "num_labels = 10 # number of possible labels\n",
    "p_dropout = 0.2 # dropout probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks remarks: \n",
    "* The lists `CL_size_filters`, `CL_num_filters`, and `MP_size` must have the same size (equal to the number of convolution layers).\n",
    "* The length of the list `FC_size` gives the number of fully-connected layers excluding the output layer. \n",
    "* The number of neurons in the output layer is equal to `num_labels`.\n",
    "\n",
    "We now load the training and test images from the MNIST dataset and normalize them to have values between -0.5 and 0.5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization function\n",
    "def normalize(image):\n",
    "    return image / 255 - 0.5\n",
    "\n",
    "# training images and labels\n",
    "train_images = normalize(mnist.train_images()[:n_train_images])\n",
    "train_labels = mnist.train_labels()[:n_train_images]\n",
    "\n",
    "# test images and labels\n",
    "test_images = normalize(mnist.test_images()[:n_test_images])\n",
    "test_labels = mnist.test_labels()[:n_test_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no file with the name `save_file_name` exists, we build it and train it. \n",
    "If it exists, we skip this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not exists(save_file_name):\n",
    "\n",
    "    # build the CNN \n",
    "    CNN = CNN3.CNN3(img_w, img_h, 1, CL_size_filters, CL_num_filters, MP_size, FC_size, num_labels)\n",
    "\n",
    "    print('MNIST CNN initialized') \n",
    "\n",
    "    # train the CNN\n",
    "\n",
    "    # number of steps elapsed \n",
    "    nsteps_l = []\n",
    "    # loss function\n",
    "    loss_l = []\n",
    "    # accuracy\n",
    "    acc_l = []\n",
    "\n",
    "    print('\\n\\n— Start of training —')\n",
    "    for epoch in range(n_epochs):\n",
    "        nsteps_l.append([])\n",
    "        loss_l.append([])\n",
    "        acc_l.append([])\n",
    "        print('\\n— Epoch {:d} of {:d} —\\n'.format(epoch+1, n_epochs))\n",
    "\n",
    "        # shuffle the training data\n",
    "        permutation = np_permutation(len(train_images))\n",
    "        train_images = train_images[permutation]\n",
    "        train_labels = train_labels[permutation]\n",
    "\n",
    "        # set the loss and number of correctly predicted labels to 0\n",
    "        loss = 0.\n",
    "        num_correct = 0\n",
    "\n",
    "        # for each image in the training set, train the CNN and update the loss and number of correctly-predicted labels\n",
    "        for i, (im, label) in enumerate(zip(train_images, train_labels)): \n",
    "            results = CNN.train(train_images[i][newaxis,:], int(train_labels[i]), learn_rate, p_dropout)\n",
    "            loss += results[0]\n",
    "            num_correct += results[1]\n",
    "            # if i is a multiple of n_print, print the loss and accuracy            \n",
    "            if i % n_print == n_print - 1:\n",
    "                nsteps_l[-1].append(i)\n",
    "                loss_l[-1].append(loss / n_print)\n",
    "                acc_l[-1].append(num_correct / n_print)\n",
    "                print('Step {:d} — average loss: {:.10f}, accuracy: {:.3f}%'.format(i+1, loss/n_print, num_correct*100./n_print))\n",
    "                # reset the loss and number of correctly-preducted labels\n",
    "                loss = 0.\n",
    "                num_correct = 0\n",
    "\n",
    "    # save the CNN\n",
    "    CNN.save(save_file_name)\n",
    "\n",
    "    # plot the evolution of the loss function\n",
    "    for e in range(n_epochs):\n",
    "        plt.plot(nsteps_l[e], loss_l[e], label='epoch {:d}'.format(e+1))\n",
    "    plt.xlim(0, n_train_images)\n",
    "    plt.xlabel(r'$n_{\\mathrm{steps}}$')\n",
    "    plt.ylabel(r'$\\mathrm{loss}$')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # plot the evolution of the accuracy\n",
    "    for e in range(n_epochs):\n",
    "        plt.plot(nsteps_l[e], acc_l[e], label='epoch {:d}'.format(e+1))\n",
    "    plt.xlim(0, n_train_images)\n",
    "    plt.xlabel(r'$n_{\\mathrm{steps}}$')\n",
    "    plt.ylabel(r'$\\mathrm{accuracy}$')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = CNN3.CNN3()\n",
    "CNN.load(save_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test the CNN on the training set and print the loss and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "— Testing the CNN —\n",
      "\n",
      "Test Loss: 0.04715147076664647\n",
      "Test Accuracy: 0.9847\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n— Testing the CNN —\\n')\n",
    "loss = 0.\n",
    "num_correct = 0\n",
    "for i, (im, label) in enumerate(zip(test_images, test_labels)):\n",
    "    results = CNN.forward_la(im[newaxis,:], int(label))\n",
    "    loss += results[0]\n",
    "    num_correct += results[1]\n",
    "print('Test Loss:', loss / n_test_images)\n",
    "print('Test Accuracy:', num_correct / n_test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is larger than 98%, which seems decent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "### Choice of hyperparameters\n",
    "\n",
    "### Comparison with a Tensorflow implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "## Appendix: The implementation\n",
    "\n",
    "### Structure\n",
    "\n",
    "### Convolutional layer\n",
    "\n",
    "### ReLU layer\n",
    "\n",
    "### Maxpool layer\n",
    "\n",
    "### Fully-connected layer\n",
    "\n",
    "### Softmax activation function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
